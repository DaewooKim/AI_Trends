# AI_Trends


# Contents
* **Transformer**
* **Non-Transformer**
* **LLM**
* **AI agent**
* **Training**


## Transformer 
### Blog
| Date | Title | Article |
|---|---|---|
| 2021.12 | A Mathematical Framework for Transformer Circuits | [homepage](https://transformer-cuitcuits.pub/2021/framework/index.html) | 
| 2023.12 | Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) | [homepage](https://lesswrong.com/) |
| 2023.12 | Fact Finding: Simplifying the Circuit (Post 2) | [homepage](https://www.lesswrong.com/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH) | 
| 2023.12 | Fact Finding: Trying to Mechanistically Understanding Early MLPs (Post 3) | [homepage](https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early) 
| 2023.12 | Fact Finding: How to Think About Interpreting Memorisation (Post 4) | [homepage](https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation) |
| 2023.12 | Fact Finding: Do Early Layers Specialise in Local Processing? (Post 5) | [homepage](https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing) | 
| 2023.10 | Generative AI exists because of the transformer | [homepage](https://ig.ft.com/generative-ai) | 

## Non-Transformer
### Paper
| Date | Title | Paper |
|---|---|---|
| 2024.07 | The Illusion of State in State-Space Models | [paper](https://arxiv.org/pdf/2404.08819) |
 
## LLM
### Paper
| Date | Title | Paper/article |
|---|---|---|
| 2024.09 | Chain of Thought Empowers Transformers to Solve Inherently Serial Problems | [paper](https://arxiv.org/pdf/2402.12875) | 
| 2024.10 | Training Language Models to Self-Correct via Reinforcement Learning | [paper](https://arxiv.org/pdf/2410.12917) |
| 2024.10 | LLMs Know More Than They Show: On The Intrinsic Representation of LLM Hallucinations| [paper](https://arxiv.org/pdf/2410.02707) |
| 2024.10 | On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability |[paper](https://arxiv.org/pdf/2409.19924) | 
| 2024.10 | Differential Transformer | [paper](https://arxiv.org/pdf/2410.05258) |
| 2024.10 | GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models | [paper](https://arxiv.org/pdf/2410.05229) |
| 2024.10 | Thinking LLMs: General Instruction Following With Thought Generation | [paper](https://arxiv.org/pdf/2410.10630) |

### Blog
| Date | Title | Paper/article |
|---|---|---|
| 2024.09 | How OpenAI's o1 changes the LLM training picture - part 1 | [homepage](https://airtain.ai/how-openai-o1-changes-the-llm-training-picture-part-1) |
| 2024.10 | How OpenAI's o1 changes the LLM training picture - part 2 | [homepage](https://airtain.ai/how-openai-o1-changes-the-llm-training-picture-part-2) |
| 2024.10 | aman.ai - OpenAI o1| [homepage](https://aman.ai/primers/ai/o1/) |

### Lectures
| Date | Title | Paper/article |
|---|---|---|
| 2024 Fall | TinyML and Efficient Deep Learning Computing (MIT) | [homepage](https://hanlab.mit.edu/courses/2024-fall-65940) |
| 2024 Fall | Large Language Models: Methods and Applications (CMU) | [homepage](https://cmu-llms.org/schedule) |

## AI Agent

### Lectures
| Date | Title | Paper/article |
|---|---|---|
| 2024.09 | Large Language Model Agents MOOC, Fall 2024 | [homepage](https://llmagents-learning.org/f24) |

## Training

### Paper
| Date | Title | Paper/article |
|---|---|---|
| 2023.12 | FP8-LM: Training FP8 Large Language Models | [paper](https://arxiv.org/pdf/2310.18313) |
| 2024.10 | Scaling FP8 training to trillion-token LLMs | [paper](https://arxiv.org/pdf/2409.12517v1) |
